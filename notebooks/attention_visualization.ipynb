# Cell 1: Import required libraries
import torch
import matplotlib.pyplot as plt
import numpy as np
from torchvision.transforms import ToPILImage
from models.radfuse_model import RadFuse
from transformers import AutoTokenizer

# Cell 2: Load the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = RadFuse()
model.load_state_dict(torch.load("path/to/checkpoint.pt", map_location=device))
model.eval().to(device)

# Cell 3: Load a sample image and report
from PIL import Image
from torchvision import transforms

image = Image.open("path/to/sample_image.png").convert("RGB")
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])
img_tensor = transform(image).unsqueeze(0).to(device)

tokenizer = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
report_text = "The lungs are clear with no evidence of pneumonia or pleural effusion."
tokens = tokenizer(report_text, return_tensors="pt", padding=True, truncation=True)
tokens = {k: v.to(device) for k, v in tokens.items()}

# Cell 4: Forward pass to extract attention (register hooks)
attn_maps = []

def hook_fn(module, input, output):
    attn_maps.append(output[1])  # Save attention weights if available

for name, module in model.fusion.named_modules():
    if isinstance(module, torch.nn.MultiheadAttention):
        module.register_forward_hook(hook_fn)

_ = model(img_tensor, tokens["input_ids"], tokens["attention_mask"])

# Cell 5: Visualize attention (dummy visualization)
if attn_maps:
    attn = attn_maps[0].detach().cpu().numpy()[0, 0]  # shape: (seq_len,)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(attn)), attn)
    plt.title("Cross-Attention from Image to Text")
    plt.xlabel("Token Index")
    plt.ylabel("Attention Weight")
    plt.show()
else:
    print("No attention map captured. Ensure forward hook is set properly.")
